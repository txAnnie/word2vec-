1. 源码：
http://word2vec.googlecode.com/svn/trunk/ 

2.编译：
下载后进入word2vec文件夹下 ‘make’ 

3.运行获得词向量：
./word2vec -train test.txt -output vectors.bin -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1

以上命令表示的是输入文件是test.txt，输出文件是vectors.bin，不使用cbow模型，默认为Skip-Gram模型。 每个单词的向量维度是200，训练的窗口大小为5就是考虑一个词前五个和后五个词语（实际代码中还有一个随机选窗口的过程，窗口大小<=5）。不使用NEG方法，使用HS方法。-sampe指的是采样的阈值，如果一个词语在训练样本中出现的频率越大，那么就越会被采样。-binary为1指的是结果二进制存储，为0是普通存储（普通存储的时候是可以打开看到词语和对应的向量的）

除了以上命令中的参数，word2vec还有几个参数对我们比较有用比如-alpha设置学习速率，默认的为0.025.–min-count设置最低频率，默认是5，如果一个词语在文档中出现的次数小于5，那么就会丢弃。

4.计算与每个词最接近的词：
./distance vectors.bin  

5.聚类：
./word2vec -train resultbig.txt -output classes.txt -cbow 0 -size 200 -window 5 -negative 0 -hs 1 -sample 1e-3 -threads 12 -classes 500  
其中，-classes设置聚类个数

6.按聚类类别排序：
sort classes.txt -k 2 -n > classes.sorted.txt 




